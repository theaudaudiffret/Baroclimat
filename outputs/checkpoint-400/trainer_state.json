{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 80.0,
  "eval_steps": 500,
  "global_step": 400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.22695035460992907,
      "grad_norm": 0.2556043863296509,
      "learning_rate": 0.0,
      "loss": 3.0514,
      "step": 1
    },
    {
      "epoch": 0.45390070921985815,
      "grad_norm": 0.248677596449852,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 3.1351,
      "step": 2
    },
    {
      "epoch": 0.6808510638297872,
      "grad_norm": 0.2471000701189041,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 3.1493,
      "step": 3
    },
    {
      "epoch": 0.9078014184397163,
      "grad_norm": 0.25009676814079285,
      "learning_rate": 5e-05,
      "loss": 3.1449,
      "step": 4
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.25471070408821106,
      "learning_rate": 6.666666666666667e-05,
      "loss": 3.0483,
      "step": 5
    },
    {
      "epoch": 1.226950354609929,
      "grad_norm": 0.25577113032341003,
      "learning_rate": 8.333333333333333e-05,
      "loss": 3.1019,
      "step": 6
    },
    {
      "epoch": 1.4539007092198581,
      "grad_norm": 0.2584363520145416,
      "learning_rate": 0.0001,
      "loss": 3.1616,
      "step": 7
    },
    {
      "epoch": 1.6808510638297873,
      "grad_norm": 0.26209020614624023,
      "learning_rate": 0.00011666666666666667,
      "loss": 3.0482,
      "step": 8
    },
    {
      "epoch": 1.9078014184397163,
      "grad_norm": 0.2580394744873047,
      "learning_rate": 0.00013333333333333334,
      "loss": 3.0347,
      "step": 9
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.27081626653671265,
      "learning_rate": 0.00015,
      "loss": 3.0317,
      "step": 10
    },
    {
      "epoch": 2.226950354609929,
      "grad_norm": 0.27577969431877136,
      "learning_rate": 0.00016666666666666666,
      "loss": 2.9908,
      "step": 11
    },
    {
      "epoch": 2.453900709219858,
      "grad_norm": 0.2786484360694885,
      "learning_rate": 0.00018333333333333334,
      "loss": 3.0239,
      "step": 12
    },
    {
      "epoch": 2.6808510638297873,
      "grad_norm": 0.27543726563453674,
      "learning_rate": 0.0002,
      "loss": 3.0162,
      "step": 13
    },
    {
      "epoch": 2.9078014184397163,
      "grad_norm": 0.26815909147262573,
      "learning_rate": 0.00021666666666666668,
      "loss": 2.9591,
      "step": 14
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.2791249752044678,
      "learning_rate": 0.00023333333333333333,
      "loss": 2.921,
      "step": 15
    },
    {
      "epoch": 3.226950354609929,
      "grad_norm": 0.26159292459487915,
      "learning_rate": 0.00025,
      "loss": 2.8731,
      "step": 16
    },
    {
      "epoch": 3.453900709219858,
      "grad_norm": 0.2629447281360626,
      "learning_rate": 0.0002666666666666667,
      "loss": 2.8994,
      "step": 17
    },
    {
      "epoch": 3.6808510638297873,
      "grad_norm": 0.2537720501422882,
      "learning_rate": 0.00028333333333333335,
      "loss": 2.8302,
      "step": 18
    },
    {
      "epoch": 3.9078014184397163,
      "grad_norm": 0.2412205934524536,
      "learning_rate": 0.0003,
      "loss": 2.8077,
      "step": 19
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.2447316199541092,
      "learning_rate": 0.00031666666666666665,
      "loss": 2.6711,
      "step": 20
    },
    {
      "epoch": 4.226950354609929,
      "grad_norm": 0.22225421667099,
      "learning_rate": 0.0003333333333333333,
      "loss": 2.791,
      "step": 21
    },
    {
      "epoch": 4.453900709219858,
      "grad_norm": 0.22128841280937195,
      "learning_rate": 0.00035,
      "loss": 2.6278,
      "step": 22
    },
    {
      "epoch": 4.680851063829787,
      "grad_norm": 0.22192060947418213,
      "learning_rate": 0.00036666666666666667,
      "loss": 2.6198,
      "step": 23
    },
    {
      "epoch": 4.907801418439716,
      "grad_norm": 0.21901185810565948,
      "learning_rate": 0.00038333333333333334,
      "loss": 2.6601,
      "step": 24
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.24975532293319702,
      "learning_rate": 0.0004,
      "loss": 2.4791,
      "step": 25
    },
    {
      "epoch": 5.226950354609929,
      "grad_norm": 0.22826842963695526,
      "learning_rate": 0.0004166666666666667,
      "loss": 2.5619,
      "step": 26
    },
    {
      "epoch": 5.453900709219858,
      "grad_norm": 0.23675519227981567,
      "learning_rate": 0.00043333333333333337,
      "loss": 2.4719,
      "step": 27
    },
    {
      "epoch": 5.680851063829787,
      "grad_norm": 0.2445361167192459,
      "learning_rate": 0.00045000000000000004,
      "loss": 2.4407,
      "step": 28
    },
    {
      "epoch": 5.907801418439716,
      "grad_norm": 0.2394075244665146,
      "learning_rate": 0.00046666666666666666,
      "loss": 2.4487,
      "step": 29
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.2687065601348877,
      "learning_rate": 0.00048333333333333334,
      "loss": 2.3012,
      "step": 30
    },
    {
      "epoch": 6.226950354609929,
      "grad_norm": 0.25705549120903015,
      "learning_rate": 0.0005,
      "loss": 2.3474,
      "step": 31
    },
    {
      "epoch": 6.453900709219858,
      "grad_norm": 0.27482330799102783,
      "learning_rate": 0.0004986486486486487,
      "loss": 2.2609,
      "step": 32
    },
    {
      "epoch": 6.680851063829787,
      "grad_norm": 0.28276312351226807,
      "learning_rate": 0.0004972972972972973,
      "loss": 2.2454,
      "step": 33
    },
    {
      "epoch": 6.907801418439716,
      "grad_norm": 0.3251023590564728,
      "learning_rate": 0.0004959459459459459,
      "loss": 2.1333,
      "step": 34
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.35063761472702026,
      "learning_rate": 0.0004945945945945946,
      "loss": 2.1788,
      "step": 35
    },
    {
      "epoch": 7.226950354609929,
      "grad_norm": 0.6343443989753723,
      "learning_rate": 0.0004932432432432432,
      "loss": 2.0913,
      "step": 36
    },
    {
      "epoch": 7.453900709219858,
      "grad_norm": 0.42161527276039124,
      "learning_rate": 0.0004918918918918919,
      "loss": 2.0437,
      "step": 37
    },
    {
      "epoch": 7.680851063829787,
      "grad_norm": 0.44744887948036194,
      "learning_rate": 0.0004905405405405405,
      "loss": 2.0079,
      "step": 38
    },
    {
      "epoch": 7.907801418439716,
      "grad_norm": 0.38749396800994873,
      "learning_rate": 0.0004891891891891892,
      "loss": 1.9289,
      "step": 39
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.4313073456287384,
      "learning_rate": 0.00048783783783783787,
      "loss": 1.7509,
      "step": 40
    },
    {
      "epoch": 8.22695035460993,
      "grad_norm": 0.4018850326538086,
      "learning_rate": 0.0004864864864864865,
      "loss": 1.912,
      "step": 41
    },
    {
      "epoch": 8.453900709219859,
      "grad_norm": 0.28793981671333313,
      "learning_rate": 0.0004851351351351351,
      "loss": 1.7599,
      "step": 42
    },
    {
      "epoch": 8.680851063829786,
      "grad_norm": 0.34013253450393677,
      "learning_rate": 0.0004837837837837838,
      "loss": 1.725,
      "step": 43
    },
    {
      "epoch": 8.907801418439716,
      "grad_norm": 0.24607501924037933,
      "learning_rate": 0.00048243243243243245,
      "loss": 1.7942,
      "step": 44
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.2617177367210388,
      "learning_rate": 0.0004810810810810811,
      "loss": 1.7192,
      "step": 45
    },
    {
      "epoch": 9.22695035460993,
      "grad_norm": 0.1949390023946762,
      "learning_rate": 0.00047972972972972974,
      "loss": 1.6395,
      "step": 46
    },
    {
      "epoch": 9.453900709219859,
      "grad_norm": 0.20240101218223572,
      "learning_rate": 0.0004783783783783784,
      "loss": 1.7182,
      "step": 47
    },
    {
      "epoch": 9.680851063829786,
      "grad_norm": 0.17125894129276276,
      "learning_rate": 0.00047702702702702703,
      "loss": 1.7264,
      "step": 48
    },
    {
      "epoch": 9.907801418439716,
      "grad_norm": 0.15504176914691925,
      "learning_rate": 0.00047567567567567573,
      "loss": 1.7068,
      "step": 49
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.16343766450881958,
      "learning_rate": 0.0004743243243243243,
      "loss": 1.5669,
      "step": 50
    },
    {
      "epoch": 10.22695035460993,
      "grad_norm": 0.1258227825164795,
      "learning_rate": 0.00047297297297297297,
      "loss": 1.6871,
      "step": 51
    },
    {
      "epoch": 10.453900709219859,
      "grad_norm": 0.12577120959758759,
      "learning_rate": 0.00047162162162162167,
      "loss": 1.6137,
      "step": 52
    },
    {
      "epoch": 10.680851063829786,
      "grad_norm": 0.11438868194818497,
      "learning_rate": 0.0004702702702702703,
      "loss": 1.5879,
      "step": 53
    },
    {
      "epoch": 10.907801418439716,
      "grad_norm": 0.11226899921894073,
      "learning_rate": 0.0004689189189189189,
      "loss": 1.6787,
      "step": 54
    },
    {
      "epoch": 11.0,
      "grad_norm": 0.1479184925556183,
      "learning_rate": 0.0004675675675675676,
      "loss": 1.6109,
      "step": 55
    },
    {
      "epoch": 11.22695035460993,
      "grad_norm": 0.10035260021686554,
      "learning_rate": 0.00046621621621621625,
      "loss": 1.6519,
      "step": 56
    },
    {
      "epoch": 11.453900709219859,
      "grad_norm": 0.09750805050134659,
      "learning_rate": 0.0004648648648648649,
      "loss": 1.6298,
      "step": 57
    },
    {
      "epoch": 11.680851063829786,
      "grad_norm": 0.09362100809812546,
      "learning_rate": 0.0004635135135135135,
      "loss": 1.6118,
      "step": 58
    },
    {
      "epoch": 11.907801418439716,
      "grad_norm": 0.09323487430810928,
      "learning_rate": 0.0004621621621621622,
      "loss": 1.5817,
      "step": 59
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.12828882038593292,
      "learning_rate": 0.0004608108108108108,
      "loss": 1.5207,
      "step": 60
    },
    {
      "epoch": 12.22695035460993,
      "grad_norm": 0.08630141615867615,
      "learning_rate": 0.00045945945945945947,
      "loss": 1.5599,
      "step": 61
    },
    {
      "epoch": 12.453900709219859,
      "grad_norm": 0.08864199370145798,
      "learning_rate": 0.0004581081081081081,
      "loss": 1.5975,
      "step": 62
    },
    {
      "epoch": 12.680851063829786,
      "grad_norm": 0.08691752701997757,
      "learning_rate": 0.00045675675675675676,
      "loss": 1.5696,
      "step": 63
    },
    {
      "epoch": 12.907801418439716,
      "grad_norm": 0.08597816526889801,
      "learning_rate": 0.0004554054054054054,
      "loss": 1.6043,
      "step": 64
    },
    {
      "epoch": 13.0,
      "grad_norm": 0.12369479984045029,
      "learning_rate": 0.0004540540540540541,
      "loss": 1.6282,
      "step": 65
    },
    {
      "epoch": 13.22695035460993,
      "grad_norm": 0.08417727053165436,
      "learning_rate": 0.0004527027027027027,
      "loss": 1.542,
      "step": 66
    },
    {
      "epoch": 13.453900709219859,
      "grad_norm": 0.08711265027523041,
      "learning_rate": 0.00045135135135135134,
      "loss": 1.5647,
      "step": 67
    },
    {
      "epoch": 13.680851063829786,
      "grad_norm": 0.0861135944724083,
      "learning_rate": 0.00045000000000000004,
      "loss": 1.6001,
      "step": 68
    },
    {
      "epoch": 13.907801418439716,
      "grad_norm": 0.08663707226514816,
      "learning_rate": 0.0004486486486486487,
      "loss": 1.5621,
      "step": 69
    },
    {
      "epoch": 14.0,
      "grad_norm": 0.12354910373687744,
      "learning_rate": 0.0004472972972972973,
      "loss": 1.5642,
      "step": 70
    },
    {
      "epoch": 14.22695035460993,
      "grad_norm": 0.0880856141448021,
      "learning_rate": 0.000445945945945946,
      "loss": 1.5398,
      "step": 71
    },
    {
      "epoch": 14.453900709219859,
      "grad_norm": 0.08165448158979416,
      "learning_rate": 0.0004445945945945946,
      "loss": 1.5772,
      "step": 72
    },
    {
      "epoch": 14.680851063829786,
      "grad_norm": 0.08029322326183319,
      "learning_rate": 0.00044324324324324327,
      "loss": 1.5505,
      "step": 73
    },
    {
      "epoch": 14.907801418439716,
      "grad_norm": 0.08505956083536148,
      "learning_rate": 0.00044189189189189186,
      "loss": 1.5585,
      "step": 74
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.125812828540802,
      "learning_rate": 0.00044054054054054056,
      "loss": 1.4677,
      "step": 75
    },
    {
      "epoch": 15.22695035460993,
      "grad_norm": 0.08642766624689102,
      "learning_rate": 0.0004391891891891892,
      "loss": 1.5239,
      "step": 76
    },
    {
      "epoch": 15.453900709219859,
      "grad_norm": 0.08779937773942947,
      "learning_rate": 0.00043783783783783785,
      "loss": 1.5519,
      "step": 77
    },
    {
      "epoch": 15.680851063829786,
      "grad_norm": 0.08833631873130798,
      "learning_rate": 0.0004364864864864865,
      "loss": 1.6163,
      "step": 78
    },
    {
      "epoch": 15.907801418439716,
      "grad_norm": 0.08579538017511368,
      "learning_rate": 0.00043513513513513514,
      "loss": 1.4078,
      "step": 79
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.1328263282775879,
      "learning_rate": 0.0004337837837837838,
      "loss": 1.5547,
      "step": 80
    },
    {
      "epoch": 16.22695035460993,
      "grad_norm": 0.0841093510389328,
      "learning_rate": 0.0004324324324324325,
      "loss": 1.4736,
      "step": 81
    },
    {
      "epoch": 16.45390070921986,
      "grad_norm": 0.0996941328048706,
      "learning_rate": 0.0004310810810810811,
      "loss": 1.5195,
      "step": 82
    },
    {
      "epoch": 16.680851063829788,
      "grad_norm": 0.09019802510738373,
      "learning_rate": 0.0004297297297297297,
      "loss": 1.5212,
      "step": 83
    },
    {
      "epoch": 16.907801418439718,
      "grad_norm": 0.08653105050325394,
      "learning_rate": 0.0004283783783783784,
      "loss": 1.5363,
      "step": 84
    },
    {
      "epoch": 17.0,
      "grad_norm": 0.13223077356815338,
      "learning_rate": 0.00042702702702702706,
      "loss": 1.4982,
      "step": 85
    },
    {
      "epoch": 17.22695035460993,
      "grad_norm": 0.08869611471891403,
      "learning_rate": 0.00042567567567567565,
      "loss": 1.5546,
      "step": 86
    },
    {
      "epoch": 17.45390070921986,
      "grad_norm": 0.09099382162094116,
      "learning_rate": 0.00042432432432432435,
      "loss": 1.5243,
      "step": 87
    },
    {
      "epoch": 17.680851063829788,
      "grad_norm": 0.09183350205421448,
      "learning_rate": 0.000422972972972973,
      "loss": 1.4616,
      "step": 88
    },
    {
      "epoch": 17.907801418439718,
      "grad_norm": 0.0870540514588356,
      "learning_rate": 0.00042162162162162164,
      "loss": 1.4128,
      "step": 89
    },
    {
      "epoch": 18.0,
      "grad_norm": 0.14810717105865479,
      "learning_rate": 0.00042027027027027023,
      "loss": 1.5667,
      "step": 90
    },
    {
      "epoch": 18.22695035460993,
      "grad_norm": 0.10319014638662338,
      "learning_rate": 0.00041891891891891893,
      "loss": 1.5038,
      "step": 91
    },
    {
      "epoch": 18.45390070921986,
      "grad_norm": 0.08997916430234909,
      "learning_rate": 0.0004175675675675676,
      "loss": 1.5558,
      "step": 92
    },
    {
      "epoch": 18.680851063829788,
      "grad_norm": 0.10028238594532013,
      "learning_rate": 0.0004162162162162162,
      "loss": 1.4354,
      "step": 93
    },
    {
      "epoch": 18.907801418439718,
      "grad_norm": 0.09303224831819534,
      "learning_rate": 0.00041486486486486487,
      "loss": 1.47,
      "step": 94
    },
    {
      "epoch": 19.0,
      "grad_norm": 0.1433071345090866,
      "learning_rate": 0.0004135135135135135,
      "loss": 1.3797,
      "step": 95
    },
    {
      "epoch": 19.22695035460993,
      "grad_norm": 0.09687940776348114,
      "learning_rate": 0.00041216216216216216,
      "loss": 1.4372,
      "step": 96
    },
    {
      "epoch": 19.45390070921986,
      "grad_norm": 0.10676096379756927,
      "learning_rate": 0.00041081081081081086,
      "loss": 1.4746,
      "step": 97
    },
    {
      "epoch": 19.680851063829788,
      "grad_norm": 0.09161990880966187,
      "learning_rate": 0.00040945945945945945,
      "loss": 1.5139,
      "step": 98
    },
    {
      "epoch": 19.907801418439718,
      "grad_norm": 0.10095595568418503,
      "learning_rate": 0.0004081081081081081,
      "loss": 1.4264,
      "step": 99
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.14176417887210846,
      "learning_rate": 0.0004067567567567568,
      "loss": 1.5033,
      "step": 100
    },
    {
      "epoch": 20.22695035460993,
      "grad_norm": 0.09906604140996933,
      "learning_rate": 0.00040540540540540544,
      "loss": 1.481,
      "step": 101
    },
    {
      "epoch": 20.45390070921986,
      "grad_norm": 0.09715952724218369,
      "learning_rate": 0.00040405405405405403,
      "loss": 1.4443,
      "step": 102
    },
    {
      "epoch": 20.680851063829788,
      "grad_norm": 0.103185273706913,
      "learning_rate": 0.00040270270270270273,
      "loss": 1.4226,
      "step": 103
    },
    {
      "epoch": 20.907801418439718,
      "grad_norm": 0.11837892979383469,
      "learning_rate": 0.0004013513513513514,
      "loss": 1.4913,
      "step": 104
    },
    {
      "epoch": 21.0,
      "grad_norm": 0.1569770872592926,
      "learning_rate": 0.0004,
      "loss": 1.383,
      "step": 105
    },
    {
      "epoch": 21.22695035460993,
      "grad_norm": 0.10210714489221573,
      "learning_rate": 0.00039864864864864866,
      "loss": 1.4119,
      "step": 106
    },
    {
      "epoch": 21.45390070921986,
      "grad_norm": 0.10692555457353592,
      "learning_rate": 0.0003972972972972973,
      "loss": 1.4343,
      "step": 107
    },
    {
      "epoch": 21.680851063829788,
      "grad_norm": 0.10163329541683197,
      "learning_rate": 0.00039594594594594595,
      "loss": 1.4679,
      "step": 108
    },
    {
      "epoch": 21.907801418439718,
      "grad_norm": 0.11274576187133789,
      "learning_rate": 0.0003945945945945946,
      "loss": 1.4428,
      "step": 109
    },
    {
      "epoch": 22.0,
      "grad_norm": 0.16452717781066895,
      "learning_rate": 0.00039324324324324324,
      "loss": 1.4317,
      "step": 110
    },
    {
      "epoch": 22.22695035460993,
      "grad_norm": 0.10334073752164841,
      "learning_rate": 0.0003918918918918919,
      "loss": 1.3887,
      "step": 111
    },
    {
      "epoch": 22.45390070921986,
      "grad_norm": 0.1049858033657074,
      "learning_rate": 0.00039054054054054053,
      "loss": 1.4432,
      "step": 112
    },
    {
      "epoch": 22.680851063829788,
      "grad_norm": 0.12081891298294067,
      "learning_rate": 0.00038918918918918923,
      "loss": 1.401,
      "step": 113
    },
    {
      "epoch": 22.907801418439718,
      "grad_norm": 0.11811311542987823,
      "learning_rate": 0.0003878378378378378,
      "loss": 1.4153,
      "step": 114
    },
    {
      "epoch": 23.0,
      "grad_norm": 0.17714141309261322,
      "learning_rate": 0.00038648648648648647,
      "loss": 1.5398,
      "step": 115
    },
    {
      "epoch": 23.22695035460993,
      "grad_norm": 0.11814826726913452,
      "learning_rate": 0.00038513513513513517,
      "loss": 1.3875,
      "step": 116
    },
    {
      "epoch": 23.45390070921986,
      "grad_norm": 0.12433423101902008,
      "learning_rate": 0.0003837837837837838,
      "loss": 1.4674,
      "step": 117
    },
    {
      "epoch": 23.680851063829788,
      "grad_norm": 0.12009205669164658,
      "learning_rate": 0.0003824324324324324,
      "loss": 1.347,
      "step": 118
    },
    {
      "epoch": 23.907801418439718,
      "grad_norm": 0.11654355376958847,
      "learning_rate": 0.0003810810810810811,
      "loss": 1.4523,
      "step": 119
    },
    {
      "epoch": 24.0,
      "grad_norm": 0.189860999584198,
      "learning_rate": 0.00037972972972972975,
      "loss": 1.3642,
      "step": 120
    },
    {
      "epoch": 24.22695035460993,
      "grad_norm": 0.10882079601287842,
      "learning_rate": 0.0003783783783783784,
      "loss": 1.3953,
      "step": 121
    },
    {
      "epoch": 24.45390070921986,
      "grad_norm": 0.11934772878885269,
      "learning_rate": 0.00037702702702702704,
      "loss": 1.4305,
      "step": 122
    },
    {
      "epoch": 24.680851063829788,
      "grad_norm": 0.12960056960582733,
      "learning_rate": 0.0003756756756756757,
      "loss": 1.4335,
      "step": 123
    },
    {
      "epoch": 24.907801418439718,
      "grad_norm": 0.12291254103183746,
      "learning_rate": 0.00037432432432432433,
      "loss": 1.3584,
      "step": 124
    },
    {
      "epoch": 25.0,
      "grad_norm": 0.187726691365242,
      "learning_rate": 0.000372972972972973,
      "loss": 1.2986,
      "step": 125
    },
    {
      "epoch": 25.22695035460993,
      "grad_norm": 0.1333233118057251,
      "learning_rate": 0.0003716216216216216,
      "loss": 1.3012,
      "step": 126
    },
    {
      "epoch": 25.45390070921986,
      "grad_norm": 0.13923557102680206,
      "learning_rate": 0.00037027027027027027,
      "loss": 1.4849,
      "step": 127
    },
    {
      "epoch": 25.680851063829788,
      "grad_norm": 0.13497115671634674,
      "learning_rate": 0.0003689189189189189,
      "loss": 1.4221,
      "step": 128
    },
    {
      "epoch": 25.907801418439718,
      "grad_norm": 0.13056594133377075,
      "learning_rate": 0.0003675675675675676,
      "loss": 1.317,
      "step": 129
    },
    {
      "epoch": 26.0,
      "grad_norm": 0.18612483143806458,
      "learning_rate": 0.0003662162162162162,
      "loss": 1.3439,
      "step": 130
    },
    {
      "epoch": 26.22695035460993,
      "grad_norm": 0.14518271386623383,
      "learning_rate": 0.00036486486486486485,
      "loss": 1.2966,
      "step": 131
    },
    {
      "epoch": 26.45390070921986,
      "grad_norm": 0.13828158378601074,
      "learning_rate": 0.00036351351351351355,
      "loss": 1.4573,
      "step": 132
    },
    {
      "epoch": 26.680851063829788,
      "grad_norm": 0.168924480676651,
      "learning_rate": 0.0003621621621621622,
      "loss": 1.2641,
      "step": 133
    },
    {
      "epoch": 26.907801418439718,
      "grad_norm": 0.143249049782753,
      "learning_rate": 0.0003608108108108108,
      "loss": 1.4658,
      "step": 134
    },
    {
      "epoch": 27.0,
      "grad_norm": 0.20674489438533783,
      "learning_rate": 0.0003594594594594595,
      "loss": 1.2807,
      "step": 135
    },
    {
      "epoch": 27.22695035460993,
      "grad_norm": 0.13573014736175537,
      "learning_rate": 0.0003581081081081081,
      "loss": 1.3684,
      "step": 136
    },
    {
      "epoch": 27.45390070921986,
      "grad_norm": 0.15724880993366241,
      "learning_rate": 0.00035675675675675677,
      "loss": 1.3208,
      "step": 137
    },
    {
      "epoch": 27.680851063829788,
      "grad_norm": 0.14534302055835724,
      "learning_rate": 0.0003554054054054054,
      "loss": 1.4134,
      "step": 138
    },
    {
      "epoch": 27.907801418439718,
      "grad_norm": 0.1524302214384079,
      "learning_rate": 0.00035405405405405406,
      "loss": 1.3484,
      "step": 139
    },
    {
      "epoch": 28.0,
      "grad_norm": 0.22298391163349152,
      "learning_rate": 0.0003527027027027027,
      "loss": 1.2346,
      "step": 140
    },
    {
      "epoch": 28.22695035460993,
      "grad_norm": 0.15200647711753845,
      "learning_rate": 0.00035135135135135135,
      "loss": 1.3568,
      "step": 141
    },
    {
      "epoch": 28.45390070921986,
      "grad_norm": 0.1876852810382843,
      "learning_rate": 0.00035,
      "loss": 1.3462,
      "step": 142
    },
    {
      "epoch": 28.680851063829788,
      "grad_norm": 0.16830232739448547,
      "learning_rate": 0.00034864864864864864,
      "loss": 1.3456,
      "step": 143
    },
    {
      "epoch": 28.907801418439718,
      "grad_norm": 0.14374400675296783,
      "learning_rate": 0.0003472972972972973,
      "loss": 1.2969,
      "step": 144
    },
    {
      "epoch": 29.0,
      "grad_norm": 0.23596809804439545,
      "learning_rate": 0.000345945945945946,
      "loss": 1.3459,
      "step": 145
    },
    {
      "epoch": 29.22695035460993,
      "grad_norm": 0.15800362825393677,
      "learning_rate": 0.0003445945945945946,
      "loss": 1.4136,
      "step": 146
    },
    {
      "epoch": 29.45390070921986,
      "grad_norm": 0.17086045444011688,
      "learning_rate": 0.0003432432432432432,
      "loss": 1.3431,
      "step": 147
    },
    {
      "epoch": 29.680851063829788,
      "grad_norm": 0.14228418469429016,
      "learning_rate": 0.0003418918918918919,
      "loss": 1.2214,
      "step": 148
    },
    {
      "epoch": 29.907801418439718,
      "grad_norm": 0.16725489497184753,
      "learning_rate": 0.00034054054054054057,
      "loss": 1.324,
      "step": 149
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.23000051081180573,
      "learning_rate": 0.00033918918918918916,
      "loss": 1.2657,
      "step": 150
    },
    {
      "epoch": 30.22695035460993,
      "grad_norm": 0.18830619752407074,
      "learning_rate": 0.00033783783783783786,
      "loss": 1.2758,
      "step": 151
    },
    {
      "epoch": 30.45390070921986,
      "grad_norm": 0.1675962656736374,
      "learning_rate": 0.0003364864864864865,
      "loss": 1.3789,
      "step": 152
    },
    {
      "epoch": 30.680851063829788,
      "grad_norm": 0.20192892849445343,
      "learning_rate": 0.00033513513513513515,
      "loss": 1.2842,
      "step": 153
    },
    {
      "epoch": 30.907801418439718,
      "grad_norm": 0.17500893771648407,
      "learning_rate": 0.0003337837837837838,
      "loss": 1.2727,
      "step": 154
    },
    {
      "epoch": 31.0,
      "grad_norm": 0.237691268324852,
      "learning_rate": 0.00033243243243243244,
      "loss": 1.3339,
      "step": 155
    },
    {
      "epoch": 31.22695035460993,
      "grad_norm": 0.15894775092601776,
      "learning_rate": 0.0003310810810810811,
      "loss": 1.2784,
      "step": 156
    },
    {
      "epoch": 31.45390070921986,
      "grad_norm": 0.17761638760566711,
      "learning_rate": 0.0003297297297297298,
      "loss": 1.32,
      "step": 157
    },
    {
      "epoch": 31.680851063829788,
      "grad_norm": 0.1724289506673813,
      "learning_rate": 0.00032837837837837837,
      "loss": 1.2792,
      "step": 158
    },
    {
      "epoch": 31.907801418439718,
      "grad_norm": 0.1766967475414276,
      "learning_rate": 0.000327027027027027,
      "loss": 1.3196,
      "step": 159
    },
    {
      "epoch": 32.0,
      "grad_norm": 0.2514306306838989,
      "learning_rate": 0.00032567567567567566,
      "loss": 1.2143,
      "step": 160
    },
    {
      "epoch": 32.226950354609926,
      "grad_norm": 0.16820158064365387,
      "learning_rate": 0.00032432432432432436,
      "loss": 1.2713,
      "step": 161
    },
    {
      "epoch": 32.45390070921986,
      "grad_norm": 0.17174354195594788,
      "learning_rate": 0.00032297297297297295,
      "loss": 1.2817,
      "step": 162
    },
    {
      "epoch": 32.680851063829785,
      "grad_norm": 0.1828499585390091,
      "learning_rate": 0.0003216216216216216,
      "loss": 1.2357,
      "step": 163
    },
    {
      "epoch": 32.90780141843972,
      "grad_norm": 0.1815042644739151,
      "learning_rate": 0.0003202702702702703,
      "loss": 1.2674,
      "step": 164
    },
    {
      "epoch": 33.0,
      "grad_norm": 0.2475631684064865,
      "learning_rate": 0.00031891891891891894,
      "loss": 1.3904,
      "step": 165
    },
    {
      "epoch": 33.226950354609926,
      "grad_norm": 0.17244891822338104,
      "learning_rate": 0.00031756756756756753,
      "loss": 1.2707,
      "step": 166
    },
    {
      "epoch": 33.45390070921986,
      "grad_norm": 0.18690428137779236,
      "learning_rate": 0.00031621621621621623,
      "loss": 1.2388,
      "step": 167
    },
    {
      "epoch": 33.680851063829785,
      "grad_norm": 0.17255568504333496,
      "learning_rate": 0.0003148648648648649,
      "loss": 1.2737,
      "step": 168
    },
    {
      "epoch": 33.90780141843972,
      "grad_norm": 0.18517032265663147,
      "learning_rate": 0.0003135135135135135,
      "loss": 1.267,
      "step": 169
    },
    {
      "epoch": 34.0,
      "grad_norm": 0.2612598240375519,
      "learning_rate": 0.00031216216216216217,
      "loss": 1.2425,
      "step": 170
    },
    {
      "epoch": 34.226950354609926,
      "grad_norm": 0.18192026019096375,
      "learning_rate": 0.0003108108108108108,
      "loss": 1.1697,
      "step": 171
    },
    {
      "epoch": 34.45390070921986,
      "grad_norm": 0.18174372613430023,
      "learning_rate": 0.00030945945945945946,
      "loss": 1.3026,
      "step": 172
    },
    {
      "epoch": 34.680851063829785,
      "grad_norm": 0.18693643808364868,
      "learning_rate": 0.00030810810810810816,
      "loss": 1.2744,
      "step": 173
    },
    {
      "epoch": 34.90780141843972,
      "grad_norm": 0.2070472240447998,
      "learning_rate": 0.00030675675675675675,
      "loss": 1.2703,
      "step": 174
    },
    {
      "epoch": 35.0,
      "grad_norm": 0.279013454914093,
      "learning_rate": 0.0003054054054054054,
      "loss": 1.1507,
      "step": 175
    },
    {
      "epoch": 35.226950354609926,
      "grad_norm": 0.17645396292209625,
      "learning_rate": 0.00030405405405405404,
      "loss": 1.313,
      "step": 176
    },
    {
      "epoch": 35.45390070921986,
      "grad_norm": 0.2132202386856079,
      "learning_rate": 0.00030270270270270274,
      "loss": 1.1669,
      "step": 177
    },
    {
      "epoch": 35.680851063829785,
      "grad_norm": 0.22816522419452667,
      "learning_rate": 0.00030135135135135133,
      "loss": 1.2252,
      "step": 178
    },
    {
      "epoch": 35.90780141843972,
      "grad_norm": 0.23111504316329956,
      "learning_rate": 0.0003,
      "loss": 1.1695,
      "step": 179
    },
    {
      "epoch": 36.0,
      "grad_norm": 0.29907357692718506,
      "learning_rate": 0.0002986486486486487,
      "loss": 1.3352,
      "step": 180
    },
    {
      "epoch": 36.226950354609926,
      "grad_norm": 0.20019249618053436,
      "learning_rate": 0.0002972972972972973,
      "loss": 1.1878,
      "step": 181
    },
    {
      "epoch": 36.45390070921986,
      "grad_norm": 0.23283541202545166,
      "learning_rate": 0.0002959459459459459,
      "loss": 1.2002,
      "step": 182
    },
    {
      "epoch": 36.680851063829785,
      "grad_norm": 0.22926725447177887,
      "learning_rate": 0.0002945945945945946,
      "loss": 1.1667,
      "step": 183
    },
    {
      "epoch": 36.90780141843972,
      "grad_norm": 0.22649343311786652,
      "learning_rate": 0.00029324324324324325,
      "loss": 1.3114,
      "step": 184
    },
    {
      "epoch": 37.0,
      "grad_norm": 0.360386461019516,
      "learning_rate": 0.0002918918918918919,
      "loss": 1.1847,
      "step": 185
    },
    {
      "epoch": 37.226950354609926,
      "grad_norm": 0.20008006691932678,
      "learning_rate": 0.00029054054054054054,
      "loss": 1.1989,
      "step": 186
    },
    {
      "epoch": 37.45390070921986,
      "grad_norm": 0.22244325280189514,
      "learning_rate": 0.0002891891891891892,
      "loss": 1.1889,
      "step": 187
    },
    {
      "epoch": 37.680851063829785,
      "grad_norm": 0.23752066493034363,
      "learning_rate": 0.00028783783783783783,
      "loss": 1.2036,
      "step": 188
    },
    {
      "epoch": 37.90780141843972,
      "grad_norm": 0.22744782269001007,
      "learning_rate": 0.00028648648648648653,
      "loss": 1.2296,
      "step": 189
    },
    {
      "epoch": 38.0,
      "grad_norm": 0.3267345130443573,
      "learning_rate": 0.0002851351351351351,
      "loss": 1.1528,
      "step": 190
    },
    {
      "epoch": 38.226950354609926,
      "grad_norm": 0.22415800392627716,
      "learning_rate": 0.00028378378378378377,
      "loss": 1.1663,
      "step": 191
    },
    {
      "epoch": 38.45390070921986,
      "grad_norm": 0.23973697423934937,
      "learning_rate": 0.0002824324324324324,
      "loss": 1.2304,
      "step": 192
    },
    {
      "epoch": 38.680851063829785,
      "grad_norm": 0.24911382794380188,
      "learning_rate": 0.0002810810810810811,
      "loss": 1.1593,
      "step": 193
    },
    {
      "epoch": 38.90780141843972,
      "grad_norm": 0.2502511441707611,
      "learning_rate": 0.0002797297297297297,
      "loss": 1.2396,
      "step": 194
    },
    {
      "epoch": 39.0,
      "grad_norm": 0.3550584614276886,
      "learning_rate": 0.00027837837837837835,
      "loss": 1.0413,
      "step": 195
    },
    {
      "epoch": 39.226950354609926,
      "grad_norm": 0.28007030487060547,
      "learning_rate": 0.00027702702702702705,
      "loss": 1.2111,
      "step": 196
    },
    {
      "epoch": 39.45390070921986,
      "grad_norm": 0.22900508344173431,
      "learning_rate": 0.0002756756756756757,
      "loss": 1.1351,
      "step": 197
    },
    {
      "epoch": 39.680851063829785,
      "grad_norm": 0.2348381131887436,
      "learning_rate": 0.0002743243243243243,
      "loss": 1.1693,
      "step": 198
    },
    {
      "epoch": 39.90780141843972,
      "grad_norm": 0.27598655223846436,
      "learning_rate": 0.000272972972972973,
      "loss": 1.1958,
      "step": 199
    },
    {
      "epoch": 40.0,
      "grad_norm": 0.3378583490848541,
      "learning_rate": 0.00027162162162162163,
      "loss": 1.0981,
      "step": 200
    },
    {
      "epoch": 40.226950354609926,
      "grad_norm": 0.24767322838306427,
      "learning_rate": 0.0002702702702702703,
      "loss": 1.1752,
      "step": 201
    },
    {
      "epoch": 40.45390070921986,
      "grad_norm": 0.25741633772850037,
      "learning_rate": 0.0002689189189189189,
      "loss": 1.194,
      "step": 202
    },
    {
      "epoch": 40.680851063829785,
      "grad_norm": 0.2682577073574066,
      "learning_rate": 0.00026756756756756756,
      "loss": 1.1292,
      "step": 203
    },
    {
      "epoch": 40.90780141843972,
      "grad_norm": 0.24605999886989594,
      "learning_rate": 0.0002662162162162162,
      "loss": 1.124,
      "step": 204
    },
    {
      "epoch": 41.0,
      "grad_norm": 0.3642992377281189,
      "learning_rate": 0.0002648648648648649,
      "loss": 1.1602,
      "step": 205
    },
    {
      "epoch": 41.226950354609926,
      "grad_norm": 0.25730830430984497,
      "learning_rate": 0.0002635135135135135,
      "loss": 1.1024,
      "step": 206
    },
    {
      "epoch": 41.45390070921986,
      "grad_norm": 0.25631779432296753,
      "learning_rate": 0.00026216216216216215,
      "loss": 1.1254,
      "step": 207
    },
    {
      "epoch": 41.680851063829785,
      "grad_norm": 0.28545820713043213,
      "learning_rate": 0.0002608108108108108,
      "loss": 1.1361,
      "step": 208
    },
    {
      "epoch": 41.90780141843972,
      "grad_norm": 0.28881675004959106,
      "learning_rate": 0.0002594594594594595,
      "loss": 1.2381,
      "step": 209
    },
    {
      "epoch": 42.0,
      "grad_norm": 0.36024534702301025,
      "learning_rate": 0.0002581081081081081,
      "loss": 1.0428,
      "step": 210
    },
    {
      "epoch": 42.226950354609926,
      "grad_norm": 0.27738291025161743,
      "learning_rate": 0.0002567567567567567,
      "loss": 1.1423,
      "step": 211
    },
    {
      "epoch": 42.45390070921986,
      "grad_norm": 0.2700555622577667,
      "learning_rate": 0.0002554054054054054,
      "loss": 1.073,
      "step": 212
    },
    {
      "epoch": 42.680851063829785,
      "grad_norm": 0.2887631058692932,
      "learning_rate": 0.00025405405405405407,
      "loss": 1.2121,
      "step": 213
    },
    {
      "epoch": 42.90780141843972,
      "grad_norm": 0.30543196201324463,
      "learning_rate": 0.00025270270270270266,
      "loss": 1.0817,
      "step": 214
    },
    {
      "epoch": 43.0,
      "grad_norm": 0.40858274698257446,
      "learning_rate": 0.00025135135135135136,
      "loss": 1.1235,
      "step": 215
    },
    {
      "epoch": 43.226950354609926,
      "grad_norm": 0.25911590456962585,
      "learning_rate": 0.00025,
      "loss": 1.1212,
      "step": 216
    },
    {
      "epoch": 43.45390070921986,
      "grad_norm": 0.2980188727378845,
      "learning_rate": 0.00024864864864864865,
      "loss": 1.1125,
      "step": 217
    },
    {
      "epoch": 43.680851063829785,
      "grad_norm": 0.2844986915588379,
      "learning_rate": 0.0002472972972972973,
      "loss": 1.0872,
      "step": 218
    },
    {
      "epoch": 43.90780141843972,
      "grad_norm": 0.2860572934150696,
      "learning_rate": 0.00024594594594594594,
      "loss": 1.136,
      "step": 219
    },
    {
      "epoch": 44.0,
      "grad_norm": 0.37538719177246094,
      "learning_rate": 0.0002445945945945946,
      "loss": 1.1091,
      "step": 220
    },
    {
      "epoch": 44.226950354609926,
      "grad_norm": 0.27928251028060913,
      "learning_rate": 0.00024324324324324326,
      "loss": 1.0541,
      "step": 221
    },
    {
      "epoch": 44.45390070921986,
      "grad_norm": 0.2823892831802368,
      "learning_rate": 0.0002418918918918919,
      "loss": 1.1147,
      "step": 222
    },
    {
      "epoch": 44.680851063829785,
      "grad_norm": 0.3019305467605591,
      "learning_rate": 0.00024054054054054055,
      "loss": 1.12,
      "step": 223
    },
    {
      "epoch": 44.90780141843972,
      "grad_norm": 0.3230692744255066,
      "learning_rate": 0.0002391891891891892,
      "loss": 1.1196,
      "step": 224
    },
    {
      "epoch": 45.0,
      "grad_norm": 0.4333990514278412,
      "learning_rate": 0.00023783783783783787,
      "loss": 1.0756,
      "step": 225
    },
    {
      "epoch": 45.226950354609926,
      "grad_norm": 0.27331846952438354,
      "learning_rate": 0.00023648648648648648,
      "loss": 1.0607,
      "step": 226
    },
    {
      "epoch": 45.45390070921986,
      "grad_norm": 0.30146077275276184,
      "learning_rate": 0.00023513513513513516,
      "loss": 1.1579,
      "step": 227
    },
    {
      "epoch": 45.680851063829785,
      "grad_norm": 0.3227073848247528,
      "learning_rate": 0.0002337837837837838,
      "loss": 1.0654,
      "step": 228
    },
    {
      "epoch": 45.90780141843972,
      "grad_norm": 0.3083595335483551,
      "learning_rate": 0.00023243243243243245,
      "loss": 1.0767,
      "step": 229
    },
    {
      "epoch": 46.0,
      "grad_norm": 0.436441570520401,
      "learning_rate": 0.0002310810810810811,
      "loss": 1.0465,
      "step": 230
    },
    {
      "epoch": 46.226950354609926,
      "grad_norm": 0.2891312539577484,
      "learning_rate": 0.00022972972972972974,
      "loss": 1.0403,
      "step": 231
    },
    {
      "epoch": 46.45390070921986,
      "grad_norm": 0.3174804747104645,
      "learning_rate": 0.00022837837837837838,
      "loss": 0.9926,
      "step": 232
    },
    {
      "epoch": 46.680851063829785,
      "grad_norm": 0.34201574325561523,
      "learning_rate": 0.00022702702702702705,
      "loss": 1.1615,
      "step": 233
    },
    {
      "epoch": 46.90780141843972,
      "grad_norm": 0.32604095339775085,
      "learning_rate": 0.00022567567567567567,
      "loss": 1.054,
      "step": 234
    },
    {
      "epoch": 47.0,
      "grad_norm": 0.44440406560897827,
      "learning_rate": 0.00022432432432432434,
      "loss": 1.1759,
      "step": 235
    },
    {
      "epoch": 47.226950354609926,
      "grad_norm": 0.3274833559989929,
      "learning_rate": 0.000222972972972973,
      "loss": 1.0033,
      "step": 236
    },
    {
      "epoch": 47.45390070921986,
      "grad_norm": 0.34492185711860657,
      "learning_rate": 0.00022162162162162163,
      "loss": 1.101,
      "step": 237
    },
    {
      "epoch": 47.680851063829785,
      "grad_norm": 0.3413960635662079,
      "learning_rate": 0.00022027027027027028,
      "loss": 1.0686,
      "step": 238
    },
    {
      "epoch": 47.90780141843972,
      "grad_norm": 0.3291032314300537,
      "learning_rate": 0.00021891891891891892,
      "loss": 1.0627,
      "step": 239
    },
    {
      "epoch": 48.0,
      "grad_norm": 0.4265505075454712,
      "learning_rate": 0.00021756756756756757,
      "loss": 1.0868,
      "step": 240
    },
    {
      "epoch": 48.226950354609926,
      "grad_norm": 0.3092414438724518,
      "learning_rate": 0.00021621621621621624,
      "loss": 1.0763,
      "step": 241
    },
    {
      "epoch": 48.45390070921986,
      "grad_norm": 0.3116782605648041,
      "learning_rate": 0.00021486486486486486,
      "loss": 1.0402,
      "step": 242
    },
    {
      "epoch": 48.680851063829785,
      "grad_norm": 0.3238169848918915,
      "learning_rate": 0.00021351351351351353,
      "loss": 1.0124,
      "step": 243
    },
    {
      "epoch": 48.90780141843972,
      "grad_norm": 0.33507224917411804,
      "learning_rate": 0.00021216216216216218,
      "loss": 1.0415,
      "step": 244
    },
    {
      "epoch": 49.0,
      "grad_norm": 0.4635726511478424,
      "learning_rate": 0.00021081081081081082,
      "loss": 1.1052,
      "step": 245
    },
    {
      "epoch": 49.226950354609926,
      "grad_norm": 0.3391762375831604,
      "learning_rate": 0.00020945945945945947,
      "loss": 1.0373,
      "step": 246
    },
    {
      "epoch": 49.45390070921986,
      "grad_norm": 0.34398022294044495,
      "learning_rate": 0.0002081081081081081,
      "loss": 1.0413,
      "step": 247
    },
    {
      "epoch": 49.680851063829785,
      "grad_norm": 0.3313645124435425,
      "learning_rate": 0.00020675675675675676,
      "loss": 1.0316,
      "step": 248
    },
    {
      "epoch": 49.90780141843972,
      "grad_norm": 0.3622278571128845,
      "learning_rate": 0.00020540540540540543,
      "loss": 1.0396,
      "step": 249
    },
    {
      "epoch": 50.0,
      "grad_norm": 0.4921160936355591,
      "learning_rate": 0.00020405405405405405,
      "loss": 1.0319,
      "step": 250
    },
    {
      "epoch": 50.226950354609926,
      "grad_norm": 0.34041252732276917,
      "learning_rate": 0.00020270270270270272,
      "loss": 1.1155,
      "step": 251
    },
    {
      "epoch": 50.45390070921986,
      "grad_norm": 0.321376234292984,
      "learning_rate": 0.00020135135135135136,
      "loss": 0.9773,
      "step": 252
    },
    {
      "epoch": 50.680851063829785,
      "grad_norm": 0.3518012464046478,
      "learning_rate": 0.0002,
      "loss": 1.0415,
      "step": 253
    },
    {
      "epoch": 50.90780141843972,
      "grad_norm": 0.3324715197086334,
      "learning_rate": 0.00019864864864864865,
      "loss": 0.9769,
      "step": 254
    },
    {
      "epoch": 51.0,
      "grad_norm": 0.48294663429260254,
      "learning_rate": 0.0001972972972972973,
      "loss": 0.9763,
      "step": 255
    },
    {
      "epoch": 51.226950354609926,
      "grad_norm": 0.3290504217147827,
      "learning_rate": 0.00019594594594594594,
      "loss": 0.9606,
      "step": 256
    },
    {
      "epoch": 51.45390070921986,
      "grad_norm": 0.34682726860046387,
      "learning_rate": 0.00019459459459459462,
      "loss": 1.1062,
      "step": 257
    },
    {
      "epoch": 51.680851063829785,
      "grad_norm": 0.34623441100120544,
      "learning_rate": 0.00019324324324324324,
      "loss": 1.0277,
      "step": 258
    },
    {
      "epoch": 51.90780141843972,
      "grad_norm": 0.3582717776298523,
      "learning_rate": 0.0001918918918918919,
      "loss": 0.9963,
      "step": 259
    },
    {
      "epoch": 52.0,
      "grad_norm": 0.5028588771820068,
      "learning_rate": 0.00019054054054054055,
      "loss": 0.9046,
      "step": 260
    },
    {
      "epoch": 52.226950354609926,
      "grad_norm": 0.3760620951652527,
      "learning_rate": 0.0001891891891891892,
      "loss": 1.0081,
      "step": 261
    },
    {
      "epoch": 52.45390070921986,
      "grad_norm": 0.3222147226333618,
      "learning_rate": 0.00018783783783783784,
      "loss": 1.0052,
      "step": 262
    },
    {
      "epoch": 52.680851063829785,
      "grad_norm": 0.3687663972377777,
      "learning_rate": 0.0001864864864864865,
      "loss": 1.0149,
      "step": 263
    },
    {
      "epoch": 52.90780141843972,
      "grad_norm": 0.3414213955402374,
      "learning_rate": 0.00018513513513513513,
      "loss": 0.9925,
      "step": 264
    },
    {
      "epoch": 53.0,
      "grad_norm": 0.5231048464775085,
      "learning_rate": 0.0001837837837837838,
      "loss": 0.9826,
      "step": 265
    },
    {
      "epoch": 53.226950354609926,
      "grad_norm": 0.34408804774284363,
      "learning_rate": 0.00018243243243243242,
      "loss": 0.9675,
      "step": 266
    },
    {
      "epoch": 53.45390070921986,
      "grad_norm": 0.3787635564804077,
      "learning_rate": 0.0001810810810810811,
      "loss": 0.9563,
      "step": 267
    },
    {
      "epoch": 53.680851063829785,
      "grad_norm": 0.35897505283355713,
      "learning_rate": 0.00017972972972972974,
      "loss": 0.9936,
      "step": 268
    },
    {
      "epoch": 53.90780141843972,
      "grad_norm": 0.3505464792251587,
      "learning_rate": 0.00017837837837837839,
      "loss": 1.0349,
      "step": 269
    },
    {
      "epoch": 54.0,
      "grad_norm": 0.5528119206428528,
      "learning_rate": 0.00017702702702702703,
      "loss": 1.0284,
      "step": 270
    },
    {
      "epoch": 54.226950354609926,
      "grad_norm": 0.36247411370277405,
      "learning_rate": 0.00017567567567567568,
      "loss": 0.9977,
      "step": 271
    },
    {
      "epoch": 54.45390070921986,
      "grad_norm": 0.3761831820011139,
      "learning_rate": 0.00017432432432432432,
      "loss": 0.9902,
      "step": 272
    },
    {
      "epoch": 54.680851063829785,
      "grad_norm": 0.4132218062877655,
      "learning_rate": 0.000172972972972973,
      "loss": 0.9253,
      "step": 273
    },
    {
      "epoch": 54.90780141843972,
      "grad_norm": 0.38947561383247375,
      "learning_rate": 0.0001716216216216216,
      "loss": 1.0151,
      "step": 274
    },
    {
      "epoch": 55.0,
      "grad_norm": 0.5116949081420898,
      "learning_rate": 0.00017027027027027028,
      "loss": 0.98,
      "step": 275
    },
    {
      "epoch": 55.226950354609926,
      "grad_norm": 0.3727027177810669,
      "learning_rate": 0.00016891891891891893,
      "loss": 0.9718,
      "step": 276
    },
    {
      "epoch": 55.45390070921986,
      "grad_norm": 0.4094386398792267,
      "learning_rate": 0.00016756756756756757,
      "loss": 0.9532,
      "step": 277
    },
    {
      "epoch": 55.680851063829785,
      "grad_norm": 0.3730822205543518,
      "learning_rate": 0.00016621621621621622,
      "loss": 0.9895,
      "step": 278
    },
    {
      "epoch": 55.90780141843972,
      "grad_norm": 0.4217071235179901,
      "learning_rate": 0.0001648648648648649,
      "loss": 0.9634,
      "step": 279
    },
    {
      "epoch": 56.0,
      "grad_norm": 0.5267264246940613,
      "learning_rate": 0.0001635135135135135,
      "loss": 0.9981,
      "step": 280
    },
    {
      "epoch": 56.226950354609926,
      "grad_norm": 0.3631206452846527,
      "learning_rate": 0.00016216216216216218,
      "loss": 0.991,
      "step": 281
    },
    {
      "epoch": 56.45390070921986,
      "grad_norm": 0.39161941409111023,
      "learning_rate": 0.0001608108108108108,
      "loss": 0.9631,
      "step": 282
    },
    {
      "epoch": 56.680851063829785,
      "grad_norm": 0.37733083963394165,
      "learning_rate": 0.00015945945945945947,
      "loss": 0.9347,
      "step": 283
    },
    {
      "epoch": 56.90780141843972,
      "grad_norm": 0.38700324296951294,
      "learning_rate": 0.00015810810810810812,
      "loss": 1.0087,
      "step": 284
    },
    {
      "epoch": 57.0,
      "grad_norm": 0.5059943795204163,
      "learning_rate": 0.00015675675675675676,
      "loss": 0.8481,
      "step": 285
    },
    {
      "epoch": 57.226950354609926,
      "grad_norm": 0.3413119316101074,
      "learning_rate": 0.0001554054054054054,
      "loss": 0.945,
      "step": 286
    },
    {
      "epoch": 57.45390070921986,
      "grad_norm": 0.3620308041572571,
      "learning_rate": 0.00015405405405405408,
      "loss": 0.9594,
      "step": 287
    },
    {
      "epoch": 57.680851063829785,
      "grad_norm": 0.38954293727874756,
      "learning_rate": 0.0001527027027027027,
      "loss": 0.9396,
      "step": 288
    },
    {
      "epoch": 57.90780141843972,
      "grad_norm": 0.37098151445388794,
      "learning_rate": 0.00015135135135135137,
      "loss": 0.9856,
      "step": 289
    },
    {
      "epoch": 58.0,
      "grad_norm": 0.5158095359802246,
      "learning_rate": 0.00015,
      "loss": 0.9127,
      "step": 290
    },
    {
      "epoch": 58.226950354609926,
      "grad_norm": 0.35717710852622986,
      "learning_rate": 0.00014864864864864866,
      "loss": 0.8741,
      "step": 291
    },
    {
      "epoch": 58.45390070921986,
      "grad_norm": 0.3653503358364105,
      "learning_rate": 0.0001472972972972973,
      "loss": 0.9339,
      "step": 292
    },
    {
      "epoch": 58.680851063829785,
      "grad_norm": 0.35863253474235535,
      "learning_rate": 0.00014594594594594595,
      "loss": 0.9189,
      "step": 293
    },
    {
      "epoch": 58.90780141843972,
      "grad_norm": 0.4040902256965637,
      "learning_rate": 0.0001445945945945946,
      "loss": 0.9935,
      "step": 294
    },
    {
      "epoch": 59.0,
      "grad_norm": 0.5315135717391968,
      "learning_rate": 0.00014324324324324327,
      "loss": 1.0803,
      "step": 295
    },
    {
      "epoch": 59.226950354609926,
      "grad_norm": 0.3425038158893585,
      "learning_rate": 0.00014189189189189188,
      "loss": 0.9611,
      "step": 296
    },
    {
      "epoch": 59.45390070921986,
      "grad_norm": 0.39316311478614807,
      "learning_rate": 0.00014054054054054056,
      "loss": 0.8928,
      "step": 297
    },
    {
      "epoch": 59.680851063829785,
      "grad_norm": 0.4172036349773407,
      "learning_rate": 0.00013918918918918917,
      "loss": 0.9337,
      "step": 298
    },
    {
      "epoch": 59.90780141843972,
      "grad_norm": 0.4051132798194885,
      "learning_rate": 0.00013783783783783785,
      "loss": 0.9647,
      "step": 299
    },
    {
      "epoch": 60.0,
      "grad_norm": 0.5513020157814026,
      "learning_rate": 0.0001364864864864865,
      "loss": 0.924,
      "step": 300
    },
    {
      "epoch": 60.226950354609926,
      "grad_norm": 0.3888199031352997,
      "learning_rate": 0.00013513513513513514,
      "loss": 0.9116,
      "step": 301
    },
    {
      "epoch": 60.45390070921986,
      "grad_norm": 0.4126947820186615,
      "learning_rate": 0.00013378378378378378,
      "loss": 0.9384,
      "step": 302
    },
    {
      "epoch": 60.680851063829785,
      "grad_norm": 0.3886703848838806,
      "learning_rate": 0.00013243243243243245,
      "loss": 0.9279,
      "step": 303
    },
    {
      "epoch": 60.90780141843972,
      "grad_norm": 0.3785664439201355,
      "learning_rate": 0.00013108108108108107,
      "loss": 0.9334,
      "step": 304
    },
    {
      "epoch": 61.0,
      "grad_norm": 0.5453450083732605,
      "learning_rate": 0.00012972972972972974,
      "loss": 0.9465,
      "step": 305
    },
    {
      "epoch": 61.226950354609926,
      "grad_norm": 0.3790183663368225,
      "learning_rate": 0.00012837837837837836,
      "loss": 0.9839,
      "step": 306
    },
    {
      "epoch": 61.45390070921986,
      "grad_norm": 0.41321372985839844,
      "learning_rate": 0.00012702702702702703,
      "loss": 0.9542,
      "step": 307
    },
    {
      "epoch": 61.680851063829785,
      "grad_norm": 0.3971922993659973,
      "learning_rate": 0.00012567567567567568,
      "loss": 0.886,
      "step": 308
    },
    {
      "epoch": 61.90780141843972,
      "grad_norm": 0.37763258814811707,
      "learning_rate": 0.00012432432432432433,
      "loss": 0.9013,
      "step": 309
    },
    {
      "epoch": 62.0,
      "grad_norm": 0.538957417011261,
      "learning_rate": 0.00012297297297297297,
      "loss": 0.8142,
      "step": 310
    },
    {
      "epoch": 62.226950354609926,
      "grad_norm": 0.40424689650535583,
      "learning_rate": 0.00012162162162162163,
      "loss": 0.9203,
      "step": 311
    },
    {
      "epoch": 62.45390070921986,
      "grad_norm": 0.38908568024635315,
      "learning_rate": 0.00012027027027027027,
      "loss": 0.9693,
      "step": 312
    },
    {
      "epoch": 62.680851063829785,
      "grad_norm": 0.40055564045906067,
      "learning_rate": 0.00011891891891891893,
      "loss": 0.9086,
      "step": 313
    },
    {
      "epoch": 62.90780141843972,
      "grad_norm": 0.40867653489112854,
      "learning_rate": 0.00011756756756756758,
      "loss": 0.8911,
      "step": 314
    },
    {
      "epoch": 63.0,
      "grad_norm": 0.5341523289680481,
      "learning_rate": 0.00011621621621621622,
      "loss": 0.8227,
      "step": 315
    },
    {
      "epoch": 63.226950354609926,
      "grad_norm": 0.3555116057395935,
      "learning_rate": 0.00011486486486486487,
      "loss": 0.8973,
      "step": 316
    },
    {
      "epoch": 63.45390070921986,
      "grad_norm": 0.3845997154712677,
      "learning_rate": 0.00011351351351351353,
      "loss": 0.8887,
      "step": 317
    },
    {
      "epoch": 63.680851063829785,
      "grad_norm": 0.41551482677459717,
      "learning_rate": 0.00011216216216216217,
      "loss": 0.9248,
      "step": 318
    },
    {
      "epoch": 63.90780141843972,
      "grad_norm": 0.41717106103897095,
      "learning_rate": 0.00011081081081081082,
      "loss": 0.9216,
      "step": 319
    },
    {
      "epoch": 64.0,
      "grad_norm": 0.5880645513534546,
      "learning_rate": 0.00010945945945945946,
      "loss": 0.8971,
      "step": 320
    },
    {
      "epoch": 64.22695035460993,
      "grad_norm": 0.41208207607269287,
      "learning_rate": 0.00010810810810810812,
      "loss": 0.9232,
      "step": 321
    },
    {
      "epoch": 64.45390070921985,
      "grad_norm": 0.42049938440322876,
      "learning_rate": 0.00010675675675675677,
      "loss": 0.8769,
      "step": 322
    },
    {
      "epoch": 64.68085106382979,
      "grad_norm": 0.41395407915115356,
      "learning_rate": 0.00010540540540540541,
      "loss": 0.9293,
      "step": 323
    },
    {
      "epoch": 64.90780141843972,
      "grad_norm": 0.39638203382492065,
      "learning_rate": 0.00010405405405405406,
      "loss": 0.8697,
      "step": 324
    },
    {
      "epoch": 65.0,
      "grad_norm": 0.5619605183601379,
      "learning_rate": 0.00010270270270270271,
      "loss": 0.931,
      "step": 325
    },
    {
      "epoch": 65.22695035460993,
      "grad_norm": 0.3963172137737274,
      "learning_rate": 0.00010135135135135136,
      "loss": 0.8843,
      "step": 326
    },
    {
      "epoch": 65.45390070921985,
      "grad_norm": 0.3918093144893646,
      "learning_rate": 0.0001,
      "loss": 0.8892,
      "step": 327
    },
    {
      "epoch": 65.68085106382979,
      "grad_norm": 0.4034214913845062,
      "learning_rate": 9.864864864864865e-05,
      "loss": 0.9375,
      "step": 328
    },
    {
      "epoch": 65.90780141843972,
      "grad_norm": 0.4231939911842346,
      "learning_rate": 9.729729729729731e-05,
      "loss": 0.9012,
      "step": 329
    },
    {
      "epoch": 66.0,
      "grad_norm": 0.5746231079101562,
      "learning_rate": 9.594594594594595e-05,
      "loss": 0.8287,
      "step": 330
    },
    {
      "epoch": 66.22695035460993,
      "grad_norm": 0.3984144926071167,
      "learning_rate": 9.45945945945946e-05,
      "loss": 0.9312,
      "step": 331
    },
    {
      "epoch": 66.45390070921985,
      "grad_norm": 0.38428953289985657,
      "learning_rate": 9.324324324324324e-05,
      "loss": 0.8587,
      "step": 332
    },
    {
      "epoch": 66.68085106382979,
      "grad_norm": 0.40498995780944824,
      "learning_rate": 9.18918918918919e-05,
      "loss": 0.888,
      "step": 333
    },
    {
      "epoch": 66.90780141843972,
      "grad_norm": 0.41420507431030273,
      "learning_rate": 9.054054054054055e-05,
      "loss": 0.8786,
      "step": 334
    },
    {
      "epoch": 67.0,
      "grad_norm": 0.5578565001487732,
      "learning_rate": 8.918918918918919e-05,
      "loss": 0.8933,
      "step": 335
    },
    {
      "epoch": 67.22695035460993,
      "grad_norm": 0.3922274708747864,
      "learning_rate": 8.783783783783784e-05,
      "loss": 0.8955,
      "step": 336
    },
    {
      "epoch": 67.45390070921985,
      "grad_norm": 0.40212252736091614,
      "learning_rate": 8.64864864864865e-05,
      "loss": 0.8723,
      "step": 337
    },
    {
      "epoch": 67.68085106382979,
      "grad_norm": 0.40795981884002686,
      "learning_rate": 8.513513513513514e-05,
      "loss": 0.8691,
      "step": 338
    },
    {
      "epoch": 67.90780141843972,
      "grad_norm": 0.40970683097839355,
      "learning_rate": 8.378378378378379e-05,
      "loss": 0.9258,
      "step": 339
    },
    {
      "epoch": 68.0,
      "grad_norm": 0.5523387789726257,
      "learning_rate": 8.243243243243245e-05,
      "loss": 0.8255,
      "step": 340
    },
    {
      "epoch": 68.22695035460993,
      "grad_norm": 0.40315625071525574,
      "learning_rate": 8.108108108108109e-05,
      "loss": 0.9026,
      "step": 341
    },
    {
      "epoch": 68.45390070921985,
      "grad_norm": 0.38501638174057007,
      "learning_rate": 7.972972972972974e-05,
      "loss": 0.885,
      "step": 342
    },
    {
      "epoch": 68.68085106382979,
      "grad_norm": 0.4013097584247589,
      "learning_rate": 7.837837837837838e-05,
      "loss": 0.8763,
      "step": 343
    },
    {
      "epoch": 68.90780141843972,
      "grad_norm": 0.41472938656806946,
      "learning_rate": 7.702702702702704e-05,
      "loss": 0.8813,
      "step": 344
    },
    {
      "epoch": 69.0,
      "grad_norm": 0.5939269661903381,
      "learning_rate": 7.567567567567568e-05,
      "loss": 0.823,
      "step": 345
    },
    {
      "epoch": 69.22695035460993,
      "grad_norm": 0.35576242208480835,
      "learning_rate": 7.432432432432433e-05,
      "loss": 0.8117,
      "step": 346
    },
    {
      "epoch": 69.45390070921985,
      "grad_norm": 0.3778109848499298,
      "learning_rate": 7.297297297297297e-05,
      "loss": 0.8775,
      "step": 347
    },
    {
      "epoch": 69.68085106382979,
      "grad_norm": 0.39213332533836365,
      "learning_rate": 7.162162162162163e-05,
      "loss": 0.9236,
      "step": 348
    },
    {
      "epoch": 69.90780141843972,
      "grad_norm": 0.45130887627601624,
      "learning_rate": 7.027027027027028e-05,
      "loss": 0.8593,
      "step": 349
    },
    {
      "epoch": 70.0,
      "grad_norm": 0.5866307020187378,
      "learning_rate": 6.891891891891892e-05,
      "loss": 0.9557,
      "step": 350
    },
    {
      "epoch": 70.22695035460993,
      "grad_norm": 0.3579481840133667,
      "learning_rate": 6.756756756756757e-05,
      "loss": 0.8662,
      "step": 351
    },
    {
      "epoch": 70.45390070921985,
      "grad_norm": 0.3886122405529022,
      "learning_rate": 6.621621621621623e-05,
      "loss": 0.8895,
      "step": 352
    },
    {
      "epoch": 70.68085106382979,
      "grad_norm": 0.39860233664512634,
      "learning_rate": 6.486486486486487e-05,
      "loss": 0.8058,
      "step": 353
    },
    {
      "epoch": 70.90780141843972,
      "grad_norm": 0.3895793557167053,
      "learning_rate": 6.351351351351352e-05,
      "loss": 0.9419,
      "step": 354
    },
    {
      "epoch": 71.0,
      "grad_norm": 0.546126663684845,
      "learning_rate": 6.216216216216216e-05,
      "loss": 0.8303,
      "step": 355
    },
    {
      "epoch": 71.22695035460993,
      "grad_norm": 0.3727729022502899,
      "learning_rate": 6.0810810810810814e-05,
      "loss": 0.9117,
      "step": 356
    },
    {
      "epoch": 71.45390070921985,
      "grad_norm": 0.37532535195350647,
      "learning_rate": 5.9459459459459466e-05,
      "loss": 0.8824,
      "step": 357
    },
    {
      "epoch": 71.68085106382979,
      "grad_norm": 0.3888530731201172,
      "learning_rate": 5.810810810810811e-05,
      "loss": 0.862,
      "step": 358
    },
    {
      "epoch": 71.90780141843972,
      "grad_norm": 0.38312622904777527,
      "learning_rate": 5.675675675675676e-05,
      "loss": 0.8413,
      "step": 359
    },
    {
      "epoch": 72.0,
      "grad_norm": 0.5664562582969666,
      "learning_rate": 5.540540540540541e-05,
      "loss": 0.8082,
      "step": 360
    },
    {
      "epoch": 72.22695035460993,
      "grad_norm": 0.3640982210636139,
      "learning_rate": 5.405405405405406e-05,
      "loss": 0.8828,
      "step": 361
    },
    {
      "epoch": 72.45390070921985,
      "grad_norm": 0.3744489252567291,
      "learning_rate": 5.2702702702702705e-05,
      "loss": 0.846,
      "step": 362
    },
    {
      "epoch": 72.68085106382979,
      "grad_norm": 0.3893730342388153,
      "learning_rate": 5.135135135135136e-05,
      "loss": 0.8586,
      "step": 363
    },
    {
      "epoch": 72.90780141843972,
      "grad_norm": 0.37768909335136414,
      "learning_rate": 5e-05,
      "loss": 0.8796,
      "step": 364
    },
    {
      "epoch": 73.0,
      "grad_norm": 0.600788414478302,
      "learning_rate": 4.8648648648648654e-05,
      "loss": 0.8463,
      "step": 365
    },
    {
      "epoch": 73.22695035460993,
      "grad_norm": 0.3507081866264343,
      "learning_rate": 4.72972972972973e-05,
      "loss": 0.8316,
      "step": 366
    },
    {
      "epoch": 73.45390070921985,
      "grad_norm": 0.3525084853172302,
      "learning_rate": 4.594594594594595e-05,
      "loss": 0.8728,
      "step": 367
    },
    {
      "epoch": 73.68085106382979,
      "grad_norm": 0.35784339904785156,
      "learning_rate": 4.4594594594594596e-05,
      "loss": 0.856,
      "step": 368
    },
    {
      "epoch": 73.90780141843972,
      "grad_norm": 0.3800574839115143,
      "learning_rate": 4.324324324324325e-05,
      "loss": 0.884,
      "step": 369
    },
    {
      "epoch": 74.0,
      "grad_norm": 0.5680888891220093,
      "learning_rate": 4.189189189189189e-05,
      "loss": 0.8562,
      "step": 370
    },
    {
      "epoch": 74.22695035460993,
      "grad_norm": 0.34304481744766235,
      "learning_rate": 4.0540540540540545e-05,
      "loss": 0.855,
      "step": 371
    },
    {
      "epoch": 74.45390070921985,
      "grad_norm": 0.37041857838630676,
      "learning_rate": 3.918918918918919e-05,
      "loss": 0.8815,
      "step": 372
    },
    {
      "epoch": 74.68085106382979,
      "grad_norm": 0.37143927812576294,
      "learning_rate": 3.783783783783784e-05,
      "loss": 0.8601,
      "step": 373
    },
    {
      "epoch": 74.90780141843972,
      "grad_norm": 0.3610019087791443,
      "learning_rate": 3.648648648648649e-05,
      "loss": 0.8215,
      "step": 374
    },
    {
      "epoch": 75.0,
      "grad_norm": 0.5698842406272888,
      "learning_rate": 3.513513513513514e-05,
      "loss": 0.8966,
      "step": 375
    },
    {
      "epoch": 75.22695035460993,
      "grad_norm": 0.3493765890598297,
      "learning_rate": 3.3783783783783784e-05,
      "loss": 0.8576,
      "step": 376
    },
    {
      "epoch": 75.45390070921985,
      "grad_norm": 0.3716530203819275,
      "learning_rate": 3.2432432432432436e-05,
      "loss": 0.8656,
      "step": 377
    },
    {
      "epoch": 75.68085106382979,
      "grad_norm": 0.3468845784664154,
      "learning_rate": 3.108108108108108e-05,
      "loss": 0.855,
      "step": 378
    },
    {
      "epoch": 75.90780141843972,
      "grad_norm": 0.3631892800331116,
      "learning_rate": 2.9729729729729733e-05,
      "loss": 0.8391,
      "step": 379
    },
    {
      "epoch": 76.0,
      "grad_norm": 0.5180697441101074,
      "learning_rate": 2.837837837837838e-05,
      "loss": 0.8819,
      "step": 380
    },
    {
      "epoch": 76.22695035460993,
      "grad_norm": 0.3423090875148773,
      "learning_rate": 2.702702702702703e-05,
      "loss": 0.8215,
      "step": 381
    },
    {
      "epoch": 76.45390070921985,
      "grad_norm": 0.3509269952774048,
      "learning_rate": 2.567567567567568e-05,
      "loss": 0.8581,
      "step": 382
    },
    {
      "epoch": 76.68085106382979,
      "grad_norm": 0.36460214853286743,
      "learning_rate": 2.4324324324324327e-05,
      "loss": 0.8448,
      "step": 383
    },
    {
      "epoch": 76.90780141843972,
      "grad_norm": 0.3601822555065155,
      "learning_rate": 2.2972972972972976e-05,
      "loss": 0.8935,
      "step": 384
    },
    {
      "epoch": 77.0,
      "grad_norm": 0.5585633516311646,
      "learning_rate": 2.1621621621621624e-05,
      "loss": 0.8615,
      "step": 385
    },
    {
      "epoch": 77.22695035460993,
      "grad_norm": 0.35740116238594055,
      "learning_rate": 2.0270270270270273e-05,
      "loss": 0.8242,
      "step": 386
    },
    {
      "epoch": 77.45390070921985,
      "grad_norm": 0.34809157252311707,
      "learning_rate": 1.891891891891892e-05,
      "loss": 0.8672,
      "step": 387
    },
    {
      "epoch": 77.68085106382979,
      "grad_norm": 0.3342856466770172,
      "learning_rate": 1.756756756756757e-05,
      "loss": 0.8911,
      "step": 388
    },
    {
      "epoch": 77.90780141843972,
      "grad_norm": 0.36465057730674744,
      "learning_rate": 1.6216216216216218e-05,
      "loss": 0.8167,
      "step": 389
    },
    {
      "epoch": 78.0,
      "grad_norm": 0.5364084243774414,
      "learning_rate": 1.4864864864864867e-05,
      "loss": 0.8829,
      "step": 390
    },
    {
      "epoch": 78.22695035460993,
      "grad_norm": 0.3476998507976532,
      "learning_rate": 1.3513513513513515e-05,
      "loss": 0.8852,
      "step": 391
    },
    {
      "epoch": 78.45390070921985,
      "grad_norm": 0.3799399733543396,
      "learning_rate": 1.2162162162162164e-05,
      "loss": 0.8936,
      "step": 392
    },
    {
      "epoch": 78.68085106382979,
      "grad_norm": 0.351399689912796,
      "learning_rate": 1.0810810810810812e-05,
      "loss": 0.8227,
      "step": 393
    },
    {
      "epoch": 78.90780141843972,
      "grad_norm": 0.35206252336502075,
      "learning_rate": 9.45945945945946e-06,
      "loss": 0.8342,
      "step": 394
    },
    {
      "epoch": 79.0,
      "grad_norm": 0.523219645023346,
      "learning_rate": 8.108108108108109e-06,
      "loss": 0.801,
      "step": 395
    },
    {
      "epoch": 79.22695035460993,
      "grad_norm": 0.3456084728240967,
      "learning_rate": 6.7567567567567575e-06,
      "loss": 0.8468,
      "step": 396
    },
    {
      "epoch": 79.45390070921985,
      "grad_norm": 0.3411276638507843,
      "learning_rate": 5.405405405405406e-06,
      "loss": 0.8808,
      "step": 397
    },
    {
      "epoch": 79.68085106382979,
      "grad_norm": 0.342183381319046,
      "learning_rate": 4.0540540540540545e-06,
      "loss": 0.8224,
      "step": 398
    },
    {
      "epoch": 79.90780141843972,
      "grad_norm": 0.34121593832969666,
      "learning_rate": 2.702702702702703e-06,
      "loss": 0.8764,
      "step": 399
    },
    {
      "epoch": 80.0,
      "grad_norm": 0.5312268137931824,
      "learning_rate": 1.3513513513513515e-06,
      "loss": 0.8029,
      "step": 400
    }
  ],
  "logging_steps": 1,
  "max_steps": 400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 100,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0137746151161856e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
