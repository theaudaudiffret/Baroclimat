# Repository Organization

```
ðŸ“‚ data/                            <-- Labeled data used for training and computing metrics
ðŸ“‚ Macro_category/                  <-- Subcategory segmentation using 5 macro categories
â”‚  â”‚â”€â”€ ðŸ“‚ Inference/                <-- CSV containing the results of the inference: text + categories
â”‚  â”‚â”€â”€ ðŸ“‚ Metrics/                  <-- CSV containing the metrics for the model
â”‚  â”‚â”€â”€ ðŸ“„ Inference.py              <-- Run this to make an inference
â”‚  â”‚â”€â”€ ðŸ“„ lora_training_MACRO.py    <-- Run this to fine-tune a model on the macro categories
â”‚
ðŸ“‚ Micro_category/                  <-- Subcategory segmentation using 25 macro categories
â”‚  â”‚â”€â”€ ðŸ“‚ Inference/                <-- CSV containing the results of the inference: text + categories
â”‚  â”‚â”€â”€ ðŸ“‚ Metrics/                  <-- CSV containing the metrics for the model
â”‚  â”‚â”€â”€ ðŸ“‚ Models_comparison/        <-- Comparison of fine-tuning different models
â”‚  â”‚   â”‚â”€â”€ ðŸ“‚ Final_model/
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“‚ inference_final_model/          <-- Inference of the 2 best models
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“‚ lora_distill_llama_8b_boost2/   <-- Inference of the 2 best models
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“‚ lora_distill_Qwen_1.5B_boost/
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“„ Benchmark.csv
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“„ resultats_base.csv
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“„ resultats_lora.csv
â”‚  â”‚   â”‚â”€â”€ ðŸ“‚ original_annotation_dataset/         <-- Comparison between multiple model inferences
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“‚ predictions_base_model/
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“‚ predictions_lora_new/
â”‚  â”‚   â”‚â”€â”€ ðŸ“„ model_comparison.ipynb               <-- Metrics comparison visualized
â”‚  â”‚â”€â”€ ðŸ“„ Inference.py               <-- Run this to make an inference
â”‚  â”‚â”€â”€ ðŸ“„ lora_training.py           <-- Run this to fine-tune a model on the micro categories
â”‚  â”‚â”€â”€ ðŸ“„ Metrics_compute_2.py
â”‚
ðŸ“‚ models/                             <-- Storage of the best models
â”‚â”€â”€ ðŸ“‚ lora_distill_llama_8b_boost/    <-- Micro category model
â”‚â”€â”€ ðŸ“‚ lora_distill_Qwen_1.5B_boost/   <-- Micro category model
â”‚â”€â”€ ðŸ“‚ Macro_lora_8B/                  <-- Macro category model
â”‚
ðŸ“‚ src/config/                        <-- Put your Hugging Face API and Azure key here
â”‚
ðŸ“‚ streamlit_app/               <-- Contains our streamlit app to vizualise the results 
â”‚
.gitignore
â”‚
Prompting_mistral_micro_classif.ipynb  <-- our prompting results 
â”‚
first_classif.ipynb  <-- Classify between climate and non climate
â”‚
Readme.md
â”‚
requirements.txt
```

The code was tested using '''Python 3.9.13'''

## Getting Started

1. Put your API key in a config file you will create at `src/config/config.json`:

```json
{
    "huggingface": {
        "api_token": "XXX"
    },
    {

    "azure": {
        "AZURE_STORAGE_CONNECTION_STRING": "XXX",
    }

    },
}
```

2. Create a virtual environment:

```sh
python3 -m venv baroclimat
source baroclimat/bin/activate
pip install -r requirements.txt
```


## ðŸ§  News Classification Models

This repository contains several models trained for news classification into thematic categories.

## ðŸ“¦ Available Models

- **`models/lora-distill-llama-8b-new`**  
  Trained on our newly labeled dataset:  
  `data/Annotations_sous_thematiques_new.csv`

- **`models/Macro-lora-8B`**  
  Trained on macro-categories derived from the new annotations:  
  `data/Annotations_macro_thematiques_new.csv`

- **`models/lora-distill-llama-8b-boost`**  
  Trained on the older annotation version:  
  `data/annotation_sous_thematiques.csv`

## ðŸ“° Data

- **`data/2024_JT_TF1_F2.csv`**  
  Contains 2024 news bulletins that we classified and used in our Streamlit app.

---

## ðŸ§ª Testing Your Trained Models

### âž¤ For 25 Categories (Micro-thematic)

```bash
python3 Micro_category/Inference.py
```

### âž¤ For 5 Categories (Macro-thematic)

```bash
python3 Macro_category/Inference_macro_1_cat.py
```

---

## ðŸ‹ï¸â€â™‚ï¸ Training a Model

You can launch training with the following scripts:

```bash
python3 Micro_category/Inference.py
```

```bash
python3 Macro_category/Inference.py
```

---

## ðŸš€ Launching the Streamlit App

To visualize results or interact with the classifier via a UI:

```bash
streamlit run streamlit_app/app.py
```

