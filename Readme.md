Repository organization:


ðŸ“‚ data/                            <-- Labeled data used for training and computing metrics
ðŸ“‚ Macro_category/                  <-- sub category segmentation using 5 macro categories
â”‚  â”‚â”€â”€ ðŸ“‚ Inference/                <-- csv containing the results of the inference: text + categories
â”‚  â”‚â”€â”€ ðŸ“‚ Metrics/                  <-- csv containing the metrics for the model
â”‚  â”‚â”€â”€ ðŸ“„ Inference.py              <-- run this to make an inference
â”‚  â”‚â”€â”€ ðŸ“„ lora_training_MACRO.py    <-- run this to fine tune a model on the macro categories
â”‚
ðŸ“‚ Micro_category/                  <-- sub category segmentation using 25 macro categories
â”‚  â”‚â”€â”€ ðŸ“‚ Inference/                <-- csv containing the results of the inference: text + categories
â”‚  â”‚â”€â”€ ðŸ“‚ Metrics/                  <-- csv containing the metrics for the model
â”‚  â”‚â”€â”€ ðŸ“‚ Models_comparison/        <-- Comparison of fine tuning different models
â”‚  â”‚   â”‚â”€â”€ ðŸ“‚ Final_model/
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“‚ inference_final_model/          <-- Inference of the 2 best models 
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“‚ lora_distill_llama_8b_boost2/   <-- Inference of the 2 best models 
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“‚ lora_distill_Qwen_1.5B_boost/
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“„ Benchmark.csv
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“„ resultats_base.csv
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“„ resultats_lora.csv
â”‚  â”‚   â”‚â”€â”€ ðŸ“‚ original_annotation_dataset/         <-- Comparison between multiple models inference
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“‚ predictions_base_model/
â”‚  â”‚   â”‚   â”‚â”€â”€ ðŸ“‚ predictions_lora_new/
â”‚  â”‚   â”‚â”€â”€ ðŸ“„ model_comparison.ipynb               <-- Metrics comparison vizualised 
â”‚  â”‚â”€â”€ ðŸ“„ Inference.py               <-- run this to make an inference
â”‚  â”‚â”€â”€ ðŸ“„ lora_training.py           <-- run this to fine tune a model on the micro categories
â”‚  â”‚â”€â”€ ðŸ“„ Metrics_compute_2.py
â”‚
ðŸ“‚ models/                             <--Storage of the best models
â”‚â”€â”€ ðŸ“‚ lora_distill_llama_8b_boost/    <--Micro category model 
â”‚â”€â”€ ðŸ“‚ lora_distill_Qwen_1.5B_boost/   <--Micro category model 
â”‚â”€â”€ ðŸ“‚ Macro_lora_8B/                  <--Macro category model 
â”‚
ðŸ“‚ Prompting_notebooks/               
ðŸ“‚ src/config/                        <--Put your hugging face api key here 
â”‚
.gitignore
â”‚
Readme.md
â”‚
requirements.txt





To get started: 




Put your API key in a config file you will create at src/config/config.json: 
{
    "huggingface": {
        "api_token": "XXX"
    }
}


Create a virtual environment: 

python3 -m venv baroclima
source baroclima/bin/activate

pip install -r requirements.txt






To test your trained models with 25 categories: 

python3 Micro_category/Inference.py


Indicate in lora_paths, save_paths, path_data 


python3 Macro_category/lora_training_MACRO.py


To do: verify paths, automation 